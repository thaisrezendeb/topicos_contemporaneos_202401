{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vaRzxrPXyIQR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "import re\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fCMIS7t82sou"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UFgDxoZL6YuI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8112b1d-f886-45b2-adb7-a2640134c841"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([50, 8, 32])\n",
            "Output shape: torch.Size([50, 8, 32])\n"
          ]
        }
      ],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2, dtype=torch.float32) * (-math.log(10000.0) / d_model)\n",
        "        )\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)  # dim 2i\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)  # dim 2i+1\n",
        "        pe = pe.unsqueeze(0)  # Shape (1, max_len, d_model)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, : x.size(1), :].to(x.device)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Exemplo\n",
        "batch_size = 8\n",
        "max_len = 50\n",
        "d_model = 32\n",
        "pos_encoding = PositionalEncoding(d_model=d_model, max_len=max_len)\n",
        "\n",
        "# (sequence_length, batch_size, d_model)\n",
        "input_tensor = torch.randn(max_len, batch_size, d_model)\n",
        "output_tensor = pos_encoding(input_tensor)\n",
        "\n",
        "print(f'Input shape: {input_tensor.shape}')  # Input shape: (sequence_length, batch_size, d_model\n",
        "print(f'Output shape: {output_tensor.shape}')  # Output shape: (sequence_length, batch_size, d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nyPw62ygyX2N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4ea72e8-b73f-4e45-91b1-01563071e4cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([8, 50, 32])\n",
            "Output shape: torch.Size([8, 50, 32])\n"
          ]
        }
      ],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        # Verifica se o número de dimensões do modelo é divisível pelo número de cabeças\n",
        "        assert d_model % num_heads == 0, \"d_model deve ser divisível por num_heads\"\n",
        "\n",
        "        # Número de dimensões por cabeça\n",
        "        self.d_k = d_model // num_heads\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        # Inicializa as camadas lineares para Q, K e V\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        # Calcula os scores fazendo o produto escalar entre Q e K e dividindo pela raiz quadrada de d_k\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "\n",
        "        # Se a máscara for fornecida, aplica a máscara para os scores\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        # Calcula a softmax nos scores\n",
        "        attention = torch.softmax(scores, dim=-1)\n",
        "\n",
        "        # Multiplica a matriz de atenção pelo valor V\n",
        "        output = torch.matmul(attention, V)\n",
        "        return output\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        # Divide a última dimensão em (num_heads, d_k)\n",
        "        N, seq_len, d_model = x.size()\n",
        "        x = x.view(N, seq_len, self.num_heads, self.d_k)\n",
        "        x = x.transpose(1, 2)  # (N, num_heads, seq_len, d_k)\n",
        "        return x\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        # Inverte a operação de split_heads\n",
        "        x = x.transpose(1, 2)  # (N, seq_len, num_heads, d_k)\n",
        "        N, seq_len, num_heads, d_k = x.size()\n",
        "        x = x.contiguous().view(N, seq_len, num_heads * d_k)\n",
        "        return x\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        # Passa os valores de Q, K e V pela camada linear\n",
        "        Q = self.split_heads(self.W_q(query))\n",
        "        K = self.split_heads(self.W_k(key))\n",
        "        V = self.split_heads(self.W_v(value))\n",
        "\n",
        "        # Calcula a atenção\n",
        "        attention = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "\n",
        "        # Combina as cabeças e aplica a camada linear final\n",
        "        output = self.combine_heads(attention)\n",
        "        output = self.W_o(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "# Exemplo\n",
        "batch_size = 8\n",
        "max_len = 50\n",
        "d_model = 32\n",
        "num_heads = 4\n",
        "multi_head_attn = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "# (batch_size, sequence_length, d_model)\n",
        "query = torch.randn(batch_size, max_len, d_model)\n",
        "key = torch.randn(batch_size, max_len, d_model)\n",
        "value = torch.randn(batch_size, max_len, d_model)\n",
        "\n",
        "output = multi_head_attn(query, key, value)\n",
        "\n",
        "print(f'Input shape: {query.shape}')  # Input shape: (batch_size, sequence_length, d_model\n",
        "print(f'Output shape: {output.shape}')  # Output shape: (batch_size, sequence_length, d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kc5gpf2V6cSY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94ec645e-bef2-40c2-a116-f0d72eee8b0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([8, 50, 32])\n",
            "Output shape: torch.Size([8, 50, 32])\n"
          ]
        }
      ],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.relu(self.fc1(x)))\n",
        "\n",
        "\n",
        "# Exemplo\n",
        "d_model = 512\n",
        "d_ff = 2048\n",
        "\n",
        "feed_forward = FeedForward(d_model, d_ff)\n",
        "\n",
        "print(f'Input shape: {query.shape}')  # Input shape: (batch_size, sequence_length, d_model)\n",
        "print(f'Output shape: {output.shape}')  # Output shape: (batch_size, sequence_length, d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ce-MVk6t6eSl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f92e8570-a6af-4b01-f339-c3b341c820ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([8, 50, 32])\n",
            "Output shape: torch.Size([8, 50, 32])\n"
          ]
        }
      ],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.ffn = FeedForward(d_model, d_ff)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        attn_output = self.attention(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = self.norm2(x + self.dropout(ffn_output))\n",
        "        return x\n",
        "\n",
        "\n",
        "# Exemplo\n",
        "batch_size = 8\n",
        "max_len = 50\n",
        "d_model = 32\n",
        "num_heads = 4\n",
        "d_ff = 128\n",
        "\n",
        "encoder_layer = EncoderLayer(d_model, num_heads, d_ff)\n",
        "\n",
        "x = torch.randn(batch_size, max_len, d_model)\n",
        "output = encoder_layer(x)\n",
        "\n",
        "print(f'Input shape: {x.shape}')  # Input shape: (batch_size, sequence_length, d_model)\n",
        "print(f'Output shape: {output.shape}')  # Output shape: (batch_size, sequence_length, d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "o5K1EaNm6gLT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "961ffcf5-b16c-4560-ef08-565519acd635"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([8, 50])\n",
            "Output shape: torch.Size([8, 50, 32])\n"
          ]
        }
      ],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(\n",
        "        self, src_vocab_size, d_model, num_heads, num_layers, d_ff, max_len, dropout=0.1\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
        "        self.layers = nn.ModuleList(\n",
        "            [EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x = self.embedding(x)\n",
        "        x = self.positional_encoding(x)\n",
        "        x = self.dropout(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return x\n",
        "\n",
        "# Exemplo\n",
        "batch_size = 8\n",
        "max_len = 50\n",
        "src_vocab_size = 1000\n",
        "d_model = 32\n",
        "num_heads = 4\n",
        "num_layers = 2\n",
        "d_ff = 128\n",
        "\n",
        "encoder = Encoder(src_vocab_size, d_model, num_heads, num_layers, d_ff, max_len)\n",
        "\n",
        "src_sequence = torch.randint(0, src_vocab_size, (batch_size, max_len))\n",
        "output = encoder(src_sequence)\n",
        "\n",
        "print(f'Input shape: {src_sequence.shape}')  # Input shape: (batch_size, sequence_length)\n",
        "print(f'Output shape: {output.shape}')  # Output shape: (batch_size, sequence_length, d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "r96t008D8z4_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22295077-0925-4fff-cd03-d99f3e2ee492"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([8, 50, 32])\n",
            "Output shape: torch.Size([8, 50, 32])\n"
          ]
        }
      ],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attention = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.cross_attention = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.ffn = FeedForward(d_model, d_ff)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_out=None, src_mask=None, trg_mask=None):\n",
        "        # Self-attention na sequência de destino\n",
        "        self_attn_output = self.self_attention(x, x, x, trg_mask)\n",
        "        x = self.norm1(x + self.dropout(self_attn_output))\n",
        "\n",
        "        # Cross-attention entre a saída do self-attention e a saída do encoder\n",
        "        if enc_out is not None:\n",
        "            cross_attn_output = self.cross_attention(x, enc_out, enc_out, src_mask)\n",
        "            x = self.norm2(x + self.dropout(cross_attn_output))\n",
        "\n",
        "        # Feed-forward\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = self.norm3(x + self.dropout(ffn_output))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# Exemplo\n",
        "batch_size = 8\n",
        "max_len = 50\n",
        "d_model = 32\n",
        "num_heads = 4\n",
        "d_ff = 128\n",
        "\n",
        "decoder_layer = DecoderLayer(d_model, num_heads, d_ff)\n",
        "\n",
        "x = torch.randn(batch_size, max_len, d_model)\n",
        "enc_out = torch.randn(batch_size, max_len, d_model)\n",
        "output = decoder_layer(x, enc_out)\n",
        "\n",
        "print(f'Input shape: {x.shape}')  # Input shape: (batch_size, sequence_length, d_model)\n",
        "print(f'Output shape: {output.shape}')  # Output shape: (batch_size, sequence_length, d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "pJGO_rj09DqG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbe76324-a3cc-4b57-b953-5c52f657fbb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([8, 50])\n",
            "Output shape: torch.Size([8, 50, 1000])\n"
          ]
        }
      ],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_len, dropout=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
        "        self.layers = nn.ModuleList(\n",
        "            [DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
        "        )\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_out=None, src_mask=None, trg_mask=None):\n",
        "        # Embedding + positional encoding + dropout\n",
        "        x = self.embedding(x)\n",
        "        x = self.positional_encoding(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Passa a entrada por cada camada do decoder\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, enc_out, src_mask, trg_mask)\n",
        "\n",
        "        # Camada final\n",
        "        out = self.fc_out(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "# Exemplo\n",
        "batch_size = 8\n",
        "max_len = 50\n",
        "vocab_size = 1000\n",
        "d_model = 32\n",
        "num_heads = 4\n",
        "num_layers = 2\n",
        "d_ff = 128\n",
        "\n",
        "decoder = Decoder(vocab_size, d_model, num_heads, num_layers, d_ff, max_len)\n",
        "\n",
        "enc_out = torch.randn(batch_size, max_len, d_model)\n",
        "tgt_sequence = torch.randint(0, vocab_size, (batch_size, max_len))\n",
        "output = decoder(tgt_sequence, enc_out)\n",
        "\n",
        "print(f'Input shape: {tgt_sequence.shape}')  # Input shape: (batch_size, sequence_length)\n",
        "print(f'Output shape: {output.shape}')  # Output shape: (batch_size, sequence_length, vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5wdvtsexCj2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d18023c-4387-4f0d-8b70-82a81a7ffa3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([8, 50])\n",
            "Output shape: torch.Size([8, 50, 1000])\n"
          ]
        }
      ],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, trg_vocab_size, d_model, num_heads, num_encoder_layers, num_decoder_layers, d_ff, max_len, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(src_vocab_size, d_model, num_heads, num_encoder_layers, d_ff, max_len, dropout)\n",
        "        self.decoder = Decoder(trg_vocab_size, d_model, num_heads, num_decoder_layers, d_ff, max_len, dropout)\n",
        "\n",
        "    def generate_mask(self, src, trg):\n",
        "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
        "        trg_mask = (trg != 0).unsqueeze(1).unsqueeze(3)\n",
        "        seq_length = trg.size(1)\n",
        "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
        "        trg_mask = trg_mask & nopeak_mask\n",
        "        return src_mask, trg_mask\n",
        "\n",
        "    def forward(self, src, trg, src_mask=None, trg_mask=None):\n",
        "        src_mask, trg_mask = self.generate_mask(src, trg)\n",
        "        enc_out = self.encoder(src, src_mask)\n",
        "        out = self.decoder(trg, enc_out, src_mask, trg_mask)\n",
        "        return out\n",
        "\n",
        "\n",
        "# Exemplo\n",
        "batch_size = 8\n",
        "max_len = 50\n",
        "src_vocab_size = 1000\n",
        "trg_vocab_size = 1000\n",
        "d_model = 32\n",
        "num_heads = 4\n",
        "num_encoder_layers = 2\n",
        "num_decoder_layers = 2\n",
        "d_ff = 128\n",
        "\n",
        "transformer = Transformer(src_vocab_size, trg_vocab_size, d_model, num_heads, num_encoder_layers, num_decoder_layers, d_ff, max_len)\n",
        "src = torch.randint(1, src_vocab_size, (batch_size, max_len))\n",
        "trg = torch.randint(1, trg_vocab_size, (batch_size, max_len))\n",
        "\n",
        "out = transformer(src, trg)\n",
        "\n",
        "print(f'Input shape: {src.shape}')  # Input shape: (batch_size, sequence_length)\n",
        "print(f'Output shape: {out.shape}')  # Output shape: (batch_size, sequence_length, trg_vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Y8TVS8WnGc84"
      },
      "outputs": [],
      "source": [
        "def create_padding_mask(seq, pad_token):\n",
        "    \"\"\"\n",
        "    Cria uma máscara de padding para sequências.\n",
        "\n",
        "    Args:\n",
        "        seq: Tensor de sequência de entrada de forma (batch_size, seq_len)\n",
        "        pad_token: O token que representa o padding no vocabulário\n",
        "\n",
        "    Retorna:\n",
        "        mask: Máscara de padding de forma (batch_size, 1, 1, seq_len)\n",
        "    \"\"\"\n",
        "    mask = (seq != pad_token).unsqueeze(1).unsqueeze(2)  # Forma: (batch_size, 1, 1, seq_len)\n",
        "    return mask\n",
        "\n",
        "def create_causal_mask(seq_len):\n",
        "    \"\"\"\n",
        "    Cria uma máscara causal para prevenir atenção a tokens futuros.\n",
        "\n",
        "    Args:\n",
        "        seq_len: O comprimento da sequência\n",
        "\n",
        "    Retorna:\n",
        "        mask: Máscara causal de forma (1, 1, seq_len, seq_len)\n",
        "    \"\"\"\n",
        "    mask = torch.tril(torch.ones((seq_len, seq_len), dtype=torch.bool))  # Forma: (seq_len, seq_len)\n",
        "    mask = mask.unsqueeze(0).unsqueeze(0)  # Forma: (1, 1, seq_len, seq_len)\n",
        "    return mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "mkSZY-CMZ_jE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d54e6f9-71da-43bf-dd78-763d45c67556"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Padding mask shape: torch.Size([8, 1, 1, 50])\n",
            "Causal mask shape: torch.Size([1, 1, 50, 50])\n"
          ]
        }
      ],
      "source": [
        "# Exemplo\n",
        "batch_size = 8\n",
        "max_len = 50\n",
        "pad_token = 0\n",
        "\n",
        "padding_mask = create_padding_mask(tgt_sequence, pad_token)\n",
        "print(f'Padding mask shape: {padding_mask.shape}')  # Padding mask shape: torch.Size([8, 1, 1, 50])\n",
        "\n",
        "causal_mask = create_causal_mask(max_len)\n",
        "print(f'Causal mask shape: {causal_mask.shape}')  # Causal mask shape: torch.Size([1, 1, 50, 50])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-2k0Sgy9F7_"
      },
      "source": [
        "## Classificador com Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "6kGAQzcxy6vN"
      },
      "outputs": [],
      "source": [
        "categories = ['sci.electronics', 'comp.graphics', 'sci.med', 'rec.motorcycles']\n",
        "\n",
        "newsgroups_data = fetch_20newsgroups(subset='all', categories=categories)\n",
        "texts = newsgroups_data.data\n",
        "labels = newsgroups_data.target\n",
        "\n",
        "texts, labels = texts[:5000], labels[:5000]\n",
        "\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    texts, labels, test_size=0.2, random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "-Kn74BYR1gq4"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    tokens = text.split()\n",
        "    return tokens\n",
        "\n",
        "def build_vocab(texts, min_freq=1):\n",
        "    word_freq = {}\n",
        "    for text in texts:\n",
        "        tokens = preprocess_text(text)\n",
        "        for token in tokens:\n",
        "            word_freq[token] = word_freq.get(token, 0) + 1\n",
        "    vocab = {'<pad>': 0, '<unk>': 1}\n",
        "    index = 2\n",
        "    for word, freq in word_freq.items():\n",
        "        if freq >= min_freq:\n",
        "            vocab[word] = index\n",
        "            index += 1\n",
        "    return vocab\n",
        "\n",
        "vocab = build_vocab(train_texts)\n",
        "vocab_size = len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "oUFyHdRsz1oQ"
      },
      "outputs": [],
      "source": [
        "class NewsGroupsDataset(Dataset):\n",
        "    def __init__(self, texts, labels, vocab, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.vocab = vocab\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def encode_text(self, text):\n",
        "        tokens = preprocess_text(text)\n",
        "        token_ids = [self.vocab.get(token, self.vocab['<unk>']) for token in tokens]\n",
        "        if len(token_ids) > self.max_len:\n",
        "            token_ids = token_ids[:self.max_len]\n",
        "        else:\n",
        "            token_ids += [self.vocab['<pad>']] * (self.max_len - len(token_ids))\n",
        "        return torch.tensor(token_ids, dtype=torch.long)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        token_ids = self.encode_text(text)\n",
        "        return token_ids, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "zGt7hHNPz1vp"
      },
      "outputs": [],
      "source": [
        "max_len = 100\n",
        "batch_size = 32\n",
        "\n",
        "train_dataset = NewsGroupsDataset(train_texts, train_labels, vocab, max_len)\n",
        "val_dataset = NewsGroupsDataset(val_texts, val_labels, vocab, max_len)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "pDm5F_uY6hPo"
      },
      "outputs": [],
      "source": [
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        src_vocab_size,\n",
        "        d_model,\n",
        "        num_heads,\n",
        "        num_layers,\n",
        "        d_ff,\n",
        "        max_len,\n",
        "        num_classes,\n",
        "        dropout=0.1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(\n",
        "            src_vocab_size, d_model, num_heads, num_layers, d_ff, max_len, dropout\n",
        "        )\n",
        "        self.classifier = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x = self.encoder(x, mask)\n",
        "        x = x.mean(dim=1)  # Global average pooling\n",
        "        logits = self.classifier(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "tqqYGFym0BEZ"
      },
      "outputs": [],
      "source": [
        "d_model = 128\n",
        "num_heads = 8\n",
        "d_ff = 512\n",
        "num_classes = len(categories)\n",
        "num_layers = 2\n",
        "dropout = 0.1\n",
        "\n",
        "model = TransformerClassifier(\n",
        "    vocab_size,\n",
        "    d_model,\n",
        "    num_heads,\n",
        "    num_layers,\n",
        "    d_ff,\n",
        "    max_len,\n",
        "    num_classes,\n",
        "    dropout,\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "_3nWLLFQyX5T"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "NVMySv_e0Ibu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5048484c-f2e0-491d-e589-2eb86429b431"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/50\n",
            "Train Loss: 0.9306\n",
            "Validation Loss: 0.8338, Accuracy: 64.89%\n",
            "\n",
            "Epoch 10/50\n",
            "Train Loss: 0.3465\n",
            "Validation Loss: 0.4784, Accuracy: 82.26%\n",
            "\n",
            "Epoch 15/50\n",
            "Train Loss: 0.1692\n",
            "Validation Loss: 0.4094, Accuracy: 86.31%\n",
            "\n",
            "Epoch 20/50\n",
            "Train Loss: 0.0801\n",
            "Validation Loss: 0.3582, Accuracy: 88.97%\n",
            "\n",
            "Epoch 25/50\n",
            "Train Loss: 0.0533\n",
            "Validation Loss: 0.3868, Accuracy: 89.48%\n",
            "\n",
            "Epoch 30/50\n",
            "Train Loss: 0.0286\n",
            "Validation Loss: 0.3645, Accuracy: 90.37%\n",
            "\n",
            "Epoch 35/50\n",
            "Train Loss: 0.0234\n",
            "Validation Loss: 0.3693, Accuracy: 91.00%\n",
            "\n",
            "Epoch 40/50\n",
            "Train Loss: 0.0173\n",
            "Validation Loss: 0.4048, Accuracy: 91.25%\n",
            "\n",
            "Epoch 45/50\n",
            "Train Loss: 0.0166\n",
            "Validation Loss: 0.4080, Accuracy: 91.25%\n",
            "\n",
            "Epoch 50/50\n",
            "Train Loss: 0.0177\n",
            "Validation Loss: 0.3818, Accuracy: 91.51%\n",
            "\n"
          ]
        }
      ],
      "source": [
        "epochs = 50\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data, labels in train_dataloader:\n",
        "        data, labels = data.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        padding_mask = create_padding_mask(data, vocab['<pad>']).to(device)\n",
        "\n",
        "        outputs = model(data, mask=padding_mask)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data, labels in val_dataloader:\n",
        "            data, labels = data.to(device), labels.to(device)\n",
        "\n",
        "            padding_mask = create_padding_mask(data, vocab['<pad>']).to(device)\n",
        "\n",
        "            outputs = model(data, mask=padding_mask)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    avg_val_loss = val_loss / len(val_dataloader)\n",
        "    val_accuracy = correct / total\n",
        "\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "        print(f\"Train Loss: {avg_train_loss:.4f}\")\n",
        "        print(f\"Validation Loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy*100:.2f}%\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xq5njmNC9Qr4"
      },
      "source": [
        "## Modelo de Linguagem com Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "IuAVw59D9faD"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"\n",
        "####-Chapter I <br>\n",
        "\n",
        "An Unexpected Party\n",
        "\n",
        "In a hole in the ground there lived a hobbit. Not a nasty, dirty, wet hole, filled with\n",
        "the ends of worms and an oozy smell, nor yet a dry, bare, sandy hole with nothing\n",
        "in it to sit down on or to eat: it was a hobbit-hole, and that means comfort.\n",
        "It  had  a  perfectly  round  door  like  a  porthole,  painted  green,  with  a  shiny\n",
        "yellow  brass  knob  in  the  exact middle. The  door  opened  on  to  a  tube-shaped hall\n",
        "like  a  tunnel:  a  very  comfortable  tunnel without  smoke, with  panelled walls,  and\n",
        "floors  tiled  and  carpeted,  provided with  polished  chairs,  and  lots  and  lots of pegs\n",
        "for  hats  and  coats - the hobbit was  fond of visitors. The  tunnel wound on  and on,\n",
        "going  fairly  but  not  quite  straight  into  the  side  of  the  hill  -  The  Hill,  as  all  the\n",
        "people for many miles round called  it - and many little round doors opened out of\n",
        "it,  first  on  one  side  and  then  on  another.  No  going  upstairs  for  the  hobbit:\n",
        "bedrooms,  bathrooms,  cellars,  pantries  (lots  of  these),  wardrobes  (he  had  whole\n",
        "rooms devoted to clothes), kitchens, dining-rooms, all were on the same floor, and\n",
        "indeed  on  the  same  passage. The  best  rooms were  all  on  the  left-hand side (going\n",
        "in),  for  these  were  the  only  ones  to  have  windows,  deep-set  round  windows\n",
        "looking over his garden and meadows beyond, sloping down to the river.\n",
        "This  hobbit  was  a  very  well-to-do  hobbit,  and  his  name  was  Baggins.  The\n",
        "Bagginses  had  lived  in  the  neighbourhood  of  The  Hill  for  time  out  of mind,  and\n",
        "people considered them very respectable, not only because most of them were rich,\n",
        "but  also  because  they  never had  any  adventures or did  anything unexpected: you\n",
        "could tell what a Baggins would say on any question without the bother of asking\n",
        "him. This  is a story of how a Baggins had an adventure,  found himself doing and\n",
        "saying things altogether unexpected. He may have lost the neighbours' respect, but\n",
        "he gained-well, you will see whether he gained anything in the end.\n",
        "The  mother  of  our  particular  hobbit  …  what  is  a  hobbit?  I  suppose  hobbits\n",
        "need  some description nowadays,  since  they have become  rare  and  shy of the Big\n",
        "People,  as  they  call  us.  They  are  (or  were)  a  little  people,  about  half  our  height,\n",
        "and  smaller  than  the  bearded Dwarves. Hobbits  have  no  beards. There  is  little  or\n",
        "no  magic  about  them,  except  the  ordinary  everyday  sort  which  helps  them  to\n",
        "disappear  quietly  and  quickly  when  large  stupid  folk  like  you  and  me  come\n",
        "blundering  along,  making  a  noise  like  elephants  which  they  can  hear  a  mile  off.\n",
        "They  are  inclined  to  be    at  in  the  stomach;  they  dress  in  bright  colours  (chiefly\n",
        "\n",
        "####-green  and  yellow);  wear  no  shoes,  because  their  feet  grow  natural  leathery  soles\n",
        "and thick warm brown hair like the stuff on their heads (which is curly); have long\n",
        "clever brown fingers, good-natured faces, and laugh deep fruity laughs (especially\n",
        "after  dinner,  which  they  have  twice  a  day when  they  can  get  it). Now  you  know\n",
        "enough  to  go  on  with.  As  I  was  saying,  the  mother  of  this  hobbit  -  of  Bilbo\n",
        "Baggins, that is  - was  the fabulous Belladonna Took, one of  the  three remarkable\n",
        "daughters  of  the  Old  Took,  head  of  the  hobbits  who  lived  across  The Water,  the\n",
        "small river that ran at the foot of The Hill. It was often said (in other families) that\n",
        "long  ago  one  of  the  Took  ancestors  must  have  taken  a  fairy  wife.  That  was,  of\n",
        "course,  absurd,  but  certainly  there  was  still  something  not  entirely  hobbit-like\n",
        "about  them,  -  and  once  in  a while members  of  the  Took-clan would go and have\n",
        "adventures. They discreetly disappeared, and  the  family hushed  it up; but  the  fact\n",
        "remained  that  the  Tooks  were  not  as  respectable  as  the  Bagginses,  though  they\n",
        "were undoubtedly richer. Not that Belladonna Took ever had any adventures after\n",
        "she  became  Mrs.  Bungo  Baggins.  Bungo,  that  was  Bilbo's  father,  built  the  most\n",
        "luxurious  hobbit-hole  for  her  (and  partly  with  her  money)  that  was  to  be  found\n",
        "either  under  The  Hill  or  over  The  Hill  or  across  The  Water,  and  there  they\n",
        "remained  to  the  end  of  their  days.  Still  it  is  probable  that  Bilbo,  her  only  son,\n",
        "although  he  looked  and  behaved  exactly  like  a  second  edition  of  his  solid  and\n",
        "comfortable  father,  got  something  a  bit  queer  in  his makeup  from  the Took  side,\n",
        "something  that  only  waited  for  a  chance  to  come  out.  The  chance  never  arrived,\n",
        "until Bilbo Baggins was grown up, being about fifty years old or so, and living in\n",
        "the  beautiful  hobbit-hole  built  by  his  father,  which  I  have  just described  for you,\n",
        "until he had in fact apparently settled down immovably.\n",
        "By some curious chance one morning long ago in the quiet of the world, when\n",
        "there  was  less  noise  and  more  green,  and  the  hobbits  were  still  numerous  and\n",
        "prosperous,  and  Bilbo  Baggins  was  standing  at  his  door  after  breakfast  smoking\n",
        "an  enormous  long  wooden  pipe  that  reached  nearly  down  to  his  woolly  toes\n",
        "(neatly brushed)  - Gandalf  came  by. Gandalf!  If  you  had  heard  only  a  quarter  of\n",
        "what  I  have  heard  about  him,  and  I  have  only  heard  very  little  of  all  there  is  to\n",
        "hear,  you  would  be  prepared  for  any  sort  I  of  remarkable  tale.  Tales  and\n",
        "adventures  sprouted  up  all  over  the  place  wherever  he  went,  in  the  most\n",
        "extraordinary  fashion.  He  had  not  been  down  that  way  under  The  Hill  for  ages\n",
        "and  ages,  not  since  his  friend  the  Old  Took  died,  in  fact,  and  the  hobbits  had\n",
        "almost forgotten what he looked like. He had been away over The Hill and across\n",
        "The  Water  on  business  of  his  own  since  they  were  all  small  hobbit-boys  and\n",
        "hobbit-girls.\n",
        "\n",
        "####-All that the unsuspecting Bilbo saw that morning was an old man with a staff.\n",
        "He had a tall pointed blue hat, a long grey cloak, a silver scarf over which a white\n",
        "beard hung down below his waist, and immense black boots.\n",
        "\"Good  morning!\"  said  Bilbo,  and  he  meant  it.  The  sun  was  shining,  and  the\n",
        "grass was very green. But Gandalf looked at him from under long bushy eyebrows\n",
        "that  stuck  out  further  than  the  brim  of  his  shady  hat.  \"What  do  you  mean?\"  be\n",
        "said. \"Do you wish me a good morning, or mean that it is a good morning whether\n",
        "I  want  not;  or  that  you  feel  good  this  morning;  or  that  it  is  morning  to  be  good\n",
        "on?\"\n",
        "\"All  of  them  at  once,\"  said  Bilbo.  \"And  a  very  fine  morning  for  a  pipe  of\n",
        "tobacco out of doors, into the bargain. If you have a pipe about you, sit down  and\n",
        "have  a  fill of mine! There's no hurry, we have  all  the day before us!\" Then Bilbo\n",
        "sat down on a seat by his door, crossed his legs, and blew out a beautiful grey ring\n",
        "of  smoke  that  sailed  up  into  the  air without  breaking  and  floated  away  over  The\n",
        "Hill.\n",
        "\"Very  pretty!\"  said  Gandalf.  \"But  I  have  no  time  to  blow  smoke-rings  this\n",
        "morning.  I  am  looking  for  someone  to  share  in  an  adventure  that  I  am  arranging,\n",
        "and it's very difficult to find anyone.\"\n",
        "«I should think so  - in these parts! We are plain quiet folk and have no use for\n",
        "adventures.  Nasty  .disturbing  uncomfortable  things! Make  you  late  for  dinner!  I\n",
        "can’t  think  what  anybody  sees  in  them,»  said  our  Mr.  Baggins,  and  stuck  one\n",
        "thumb  behind  his  braces,  and  blew  out  another  even  bigger  smoke-ring. Then he\n",
        "took out his morning  letters, and begin  to  read, pretending  to  take no more notice\n",
        "of  the old man. He had decided  that he was not quite his  sort,  and wanted him  to\n",
        "go away. But  the old man did not move. He stood  leaning on his stick and gazing\n",
        "at the hobbit without saying anything, till Bilbo got quite uncomfortable and even\n",
        "a little cross.\n",
        "\"Good  morning!\"  he  said  at  last.  \"We  don't  want  any  adventures  here,  thank\n",
        "you! You might try over The Hill or across The Water.\" By this he meant that the\n",
        "conversation was at an end.\n",
        "\"What a lot of things you do use Good morning for!\" said Gandalf. \"Now you\n",
        "mean that you want to get rid of me, and that it won't be good till I move off.\"\n",
        "\"Not  at  all,  not  at  all,  my  dear  sir!  Let  me  see,  I  don't  think  I  know  your\n",
        "name?\"\n",
        "\"Yes,  yes,  my  dear  sir  -  and  I  do  know  your  name, Mr.  Bilbo  Baggins.  And\n",
        "you  do  know  my  name,  though  you  don't  remember  that  I  belong  to  it.  I  am\n",
        "\n",
        "####-Gandalf,  and  Gandalf  means  me!  To  think  that  I  should  have  lived  to  be  good-\n",
        "morninged by Belladonna Took's son, as if I was selling buttons at the door!\"\n",
        "\"Gandalf,  Gandalf!  Good  gracious  me!  Not  the  wandering  wizard  that  gave\n",
        "Old Took a pair of magic diamond studs that fastened themselves and never came\n",
        "undone  till  ordered?  Not  the  fellow  who  used  to  tell  such  wonderful  tales  at\n",
        "parties, about dragons and goblins and giants and  the rescue of princesses and  the\n",
        "unexpected  luck  of  widows'  sons?  Not  the  man  that  used  to  make  such\n",
        "particularly  excellent  fireworks!  I  remember  those!  Old  Took  used  to  have  them\n",
        "on  Midsummer's  Eve.  Splendid!  They  used  to  go  up  like  great  lilies  and\n",
        "snapdragons and laburnums of fire and hang in the twilight all evening!\" You will\n",
        "notice already that Mr. Baggins was not quite so prosy as he liked to believe, also\n",
        "that  he was  very  fond  of  flowers. \"Dear me!\" she went on. \"Not  the Gandalf who\n",
        "was  responsible  for so many quiet  lads and  lasses going off  into  the Blue  for mad\n",
        "adventures.  Anything  from  climbing  trees  to  visiting  Elves  -  or  sailing  in  ships,\n",
        "sailing  to other  shores! Bless me,  life used  to be quite  inter  - I mean, you used  to\n",
        "upset things badly in these parts once upon a time. I beg your pardon, but I had no\n",
        "idea you were still in business.\"\n",
        "\"Where else  should  I be?\"  said  the wizard.  \"All  the  same  I am pleased  to  find\n",
        "you  remember  something about me. You seem  to  remember my  fireworks kindly,\n",
        "at any  rate,  land  that  is not without hope.  Indeed  for your old grand-father Took's\n",
        "sake, and for the sake of poor Belladonna, I will give you what you asked for.\"\n",
        "\"I beg your pardon, I haven't asked for anything!\"\n",
        "\"Yes, you have! Twice now. My pardon. I give it you. In fact I will go so far as\n",
        "to  send  you  on  this  adventure.  Very  amusing  for  me,  very  good  for  you  and\n",
        "profitable too, very likely, if you ever get over it.\"\n",
        "\"Sorry!  I  don't  want  any  adventures,  thank  you.  Not  today.  Good  morning!\n",
        "But please come to tea - any time you like! Why not tomorrow? Come tomorrow!\n",
        "Good-bye!\"\n",
        "With  that  the hobbit  turned and  scuttled  inside his  round green door, and  shut\n",
        "it as quickly as he dared, not to seen rude. Wizards after all are wizards.\n",
        "\"What on earth did I ask him to tea for!\" he said to him-self, as he went to the\n",
        "pantry. He had only  just had break  fast, but he  thought a cake or  two and a drink\n",
        "of  something  would  do  him  good  after  his  fright.  Gandalf  in  the  meantime  was\n",
        "still  standing  outside  the  door,  and  laughing  long  but  quietly.  After  a  while  he\n",
        "stepped  up,  and  with  the  spike  of  his  staff  scratched  a  queer  sign  on  the  hobbit's\n",
        "beautiful  green  front-door.  Then  he  strode  away,  just  about  the  time  when  Bilbo\n",
        "\n",
        "####-was  finishing  his  second  cake  and  beginning  to  think  that  he  had  escape\n",
        "adventures very well.\n",
        "The  next  day  he  had  almost  forgotten  about  Gandalf  He  did  not  remember\n",
        "things  very  well,  unless  he  put  them  down  on  his  Engagement  Tablet:  like  this:\n",
        "Gandalf ’¥a Wednesday. Yesterday he had been too flustered to do anything of the\n",
        "kind. Just before tea-time there came a tremendous ring on the front-door bell, and\n",
        "then he remembered! He rushed and put on the kettle, and put out another cup and\n",
        "saucer and an extra cake or two, and ran to the door.\n",
        "\"I am so sorry  to keep you waiting!\" he was going  to say, when he saw  that  it\n",
        "was not Gandalf at all. It was a dwarf with a blue beard tucked into a golden belt,\n",
        "and  very  bright  eyes  under  his  dark-green hood.  As  soon a  the door was opened,\n",
        "he pushed inside, just as if he had been expected.\n",
        "He hung his hooded cloak on the nearest peg, and \"Dwalin at your service!\" he\n",
        "said with a low bow.\n",
        "\"Bilbo  Baggins  at  yours!\"  said  the  hobbit,  too  surprised  to  ask  any  questions\n",
        "for  the  moment.  When  the  silence  that  followed  had  become  uncomfortable,  he\n",
        "added:  \"I  am  just  about  to  take  tea;  pray  come  and  have  some with me.\" A  little\n",
        "stiff  perhaps,  but  he  meant  it  kindly.  And  what  would  you  do,  if  an  uninvited\n",
        "dwarf came and hung his things up in your hall without a word of explanation?\n",
        "They had not been at table long, in fact they had hardly reached the third cake,\n",
        "when there came another even louder ring at the bell.\n",
        "\"Excuse me!\" said the hobbit, and off he went to the door.\n",
        "\"So  you  have  got  here  at  last!\" was what  he was  going  to  say  to Gandalf  this\n",
        "time.  But  it  was  not  Gandalf.  Instead  there  was  a  very  old-looking  dwarf  on  the\n",
        "step with a white beard and a scarlet hood; and he too hopped inside as soon as the\n",
        "door was open, just as if he had been invited.\n",
        "\"I  see  they  have  begun  to  arrive  already,\"  he  said  when  he  caught  sight  of\n",
        "Dwalin's  green  hood  hanging  up.  He  hung  his  red  one  next  to  it,  and  \"Balin  at\n",
        "your service!\" he said with his hand on his breast.\n",
        "\"Thank  you!\"  said  Bilbo with  a  gasp.  It was  not  the  correct  thing  to  say,  but\n",
        "they have begun  to  arrive had  flustered him badly. He  liked visitors, but he  liked\n",
        "to know  them before  they arrived, and he preferred  to ask  them himself. He had a\n",
        "horrible  thought  that  the  cakes might  run  short,  and  then  he-as  the host: he knew\n",
        "his duty and stuck to it however painful-he might have to go without.\n",
        "\"Come  along  in,  and  have  some  tea!\"  he  managed  to  say  after  taking  a  deep\n",
        "breath.\n",
        "\n",
        "####-\"A  little  beer  would  suit  me  better,  if  it  is  all  the  same  to  you, my  good  sir,\"\n",
        "said  Balin  with  the  white  beard.  \"But  I  don't  mind  some  cake-seed-cake,  if  you\n",
        "have any.\"\n",
        "\"Lots!\"  Bilbo  found  himself  answering,  to  his  own  surprise;  and  he  found\n",
        "himself  scuttling  off,  too,  to  the  cellar  to  fill  a  pint  beer-mug, and to the pantry to\n",
        "fetch  two  beautiful  round  seed-cakes  which  he  had  baked  that  afternoon  for  his\n",
        "after-supper morsel.\n",
        "When he got back Balin and Dwalin were  talking at  the  table  like old  friends\n",
        "(as  a  matter  of  fact  they  were  brothers).  Bilbo  plumped  down  the  beer  and  the\n",
        "cake  in  front  of  them, when  loud  came  a  ring  at  the  bell  again,  and  then  another\n",
        "ring.\n",
        "\"Gandalf for certain this time,\" he thought as he puffed along the passage. But\n",
        "it was not. It was two more dwarves, both with blue hoods, silver belts, and yellow\n",
        "beards;  and  each  of  them  carried  a  bag  of  tools  and  a  spade.  In  they  hopped,  as\n",
        "soon as the door began to open-Bilbo was hardly surprised at all.\n",
        "\"What can I do for you, my dwarves?\" he said. \"Kili at your service!\" said the\n",
        "one.  \"And  Fili!\"  added  the  other;  and  they  both  swept  off  their  blue  hoods  and\n",
        "bowed.\n",
        "\"At  yours  and  your  family's!\"  replied  Bilbo,  remembering  his  manners  this\n",
        "time.\n",
        "\"Dwalin and Balin here already, I see,\" said Kili. \"Let us join the throng!\"\n",
        "\"Throng!\" thought Mr. Baggins. \"I don't like the sound of that. I really must sit\n",
        "down for a minute and collect my wits, and have a drink.\" He had only just had a\n",
        "sip-in  the  corner,  while  the  four  dwarves  sat  around  the  table,  and  talked  about\n",
        "mines  and  gold  and  troubles  with  the  goblins,  and  the  depredations  of  dragons,\n",
        "and lots of other things which he did not understand, and did not want to, for they\n",
        "sounded much too adventurous-when, ding-dong-a-ling-' dang, his bell rang again,\n",
        "as if some naughty little hobbit-boy was trying to pull the handle off. \"Someone at\n",
        "the  door!\"  he  said,  blinking.  \"Some  four,  I  should  say  by  the  sound,\"  said  Fili.\n",
        "\"Be-sides, we saw them coming along behind us in the distance.\"\n",
        "The  poor  little  hobbit  sat  down  in  the  hall  and  put  his  head  in  his  hands,  and\n",
        "wondered  what  had  happened,  and  what  was  going  to  happen,  and  whether  they\n",
        "would all stay  to supper. Then  the bell rang again  louder  than ever, and he had  to\n",
        "run  to  the  door.  It  was  not  four  after  all,  t  was  FIVE.  Another  dwarf  had  come\n",
        "along while he was wondering in the hall. He had hardly turned the knob, be-x)re\n",
        "they were all  inside, bowing and  saying  \"at your  service\" one after another. Dori,\n",
        "Nori,  Ori,  Oin,  and  Gloin  were  their  names;  and  very  soon  two  purple  hoods,  a\n",
        "\n",
        "####-grey hood, a brown hood, and a white hood were hanging on the pegs, and off they\n",
        "marched  with  their  broad  hands  stuck  in  their  gold  and  silver  belts  to  join  the\n",
        "others. Already  it had almost become a  throng. Some called  for ale, and some  for\n",
        "porter,  and  one  for  coffee,  and  all  of  them  for  cakes;  so  the hobbit was kept very\n",
        "busy for a while.\n",
        "A  big  jug  of  coffee  bad  just  been  set  in  the  hearth,  the  seed-cakes were gone,\n",
        "and  the  dwarves  were  starting  on  a  round  of  buttered  scones,  when  there  came-a\n",
        "loud  knock.  Not  a  ring,  but  a  hard  rat-tat  on  the  hobbit's  beautiful  green  door.\n",
        "Somebody was banging with a stick!\n",
        "Bilbo  rushed  along  the  passage,  very  angry,  and  altogether  bewildered  and\n",
        "bewuthered-this  was  the  most  awkward  Wednesday  he  ever  remembered.  He\n",
        "pulled open the door with a jerk, and they all fell in, one on top of the other. More\n",
        "dwarves,  four  more!  And  there  was  Gandalf  behind,  leaning  on  his  staff  and\n",
        "laughing. He had made quite a dent on the beautiful door; he had also, by the way,\n",
        "knocked out the secret mark that he had put there the morning before.\n",
        "\"Carefully!  Carefully!\"  he  said.  \"It  is  not  like  you,  Bilbo,  to  keep  friends\n",
        "waiting  on  the  mat,  and  then  open  the  door  like  a  pop-gun!  Let  me  introduce\n",
        "Bifur, Bofur, Bombur, and especially Thorin!\"\n",
        "\"At your service!\" said Bifur, Bofur, and Bombur standing in a row. Then they\n",
        "hung  up  two  yellow  hoods  and  a  pale  green  one;  and  also  a  sky-blue one with  a\n",
        "long silver tassel. This last belonged to Thorin, an enormously important dwarf, in\n",
        "fact  no  other  than  the  great  Thorin  Oakenshield  himself,  who  was  not  at  all\n",
        "pleased at falling flat on Bilbo's mat with Bifur, Bofur, and Bombur on top of him.\n",
        "For  one  thing  Bombur  was  immensely  fat  and  heavy.  Thorin  indeed  was  very\n",
        "haughty,  and  said  nothing  about  service;  but  poor Mr. Baggins  said  he was  sorry\n",
        "so  many  times,  that  at  last  he  grunted  \"pray  don't  mention  it,\"  and  stopped\n",
        "frowning.\n",
        "\"Now we  are  all here!\"  said Gandalf,  looking  at  the  row of  thirteen hoods-the\n",
        "best  detachable  party  hoods-and his own hat hanging on  the pegs. \"Quite a merry\n",
        "gathering!\n",
        "I hope there is something left for the late-comers to eat and drink! What's that?\n",
        "Tea! No thank you! A little red wine, I think, for me.\" \"And for me,\" said Thorin.\n",
        "\"And raspberry jam and apple-tart,\" said Bifur. \"And mince-pies and cheese,\" said\n",
        "Bofur.  \"And  pork-pie  and  salad,\"  said  Bombur.  \"And  more  cakes-and  ale-and\n",
        "coffee, if you don't mind,\" called the other dwarves through the door.\n",
        "\n",
        "####-\"Put  on  a  few  eggs,  there's  a  good  fellow!\"  Gandalf  called  after  him,  as  the\n",
        "hobbit  stumped  off  to  the  pantries.  \"And  just  bring  out  the  cold  chicken  and\n",
        "pickles!\"\n",
        "\"Seems  to  know  as  much  about  the  inside  of  my  larders  as  I  do  myself!\"\n",
        "thought Mr. Baggins, who was  feeling  positively  flummoxed,  and was  beginning\n",
        "to wonder whether  a most wretched  adventure  had  not  come  right  into his house.\n",
        "By  the  time he had got all  the bottles and dishes and knives and forks and glasses\n",
        "and  plates  and  spoons  and  things  piled  up  on  big  trays,  he  was  getting  very  hot,\n",
        "and red in the face, and annoyed.\n",
        "\"Confusticate  and  bebother  these  dwarves!\"  he  said  aloud.  \"Why  don't  they\n",
        "come and lend a hand?\" Lo and behold! there stood Balin and Dwalin at the door\n",
        "of  the  kitchen,  and  Fili  and Kili  behind  them,  and  before  he  could  say  knife  they\n",
        "had  whisked  the  trays  and  a  couple  of  small  tables  into  the  parlour  and  set  out\n",
        "everything afresh.\n",
        "Gandalf  sat  at  the  head  of  the  party with  the  thirteen,  dwarves  all  round:  and\n",
        "Bilbo  sat  on  a  stool  at  the  fireside,  nibbling  at  a  biscuit  (his  appetite  was  quite\n",
        "taken away), and trying to look as if this was all perfectly  ordinary and. not in the\n",
        "least  an  adventure. The  dwarves  ate  and  ate,  and  talked  and  talked,  and  time  got\n",
        "on.  At  last  they  pushed  their  chairs  back,  and  Bilbo  made  a  move  to  collect  the\n",
        "plates and glasses.\n",
        "\"I suppose you will all stay to supper?\" he said in his politest unpressing tones.\n",
        "\"Of  course!\"  said Thorin.  \"And  after. We  shan't  get  through  the  business  till  late,\n",
        "and we must have some music first. Now to clear up!\"\n",
        "Thereupon  the  twelve  dwarves-not  Thorin,  he  was  too  important,  and  stayed\n",
        "talking  to  Gandalf-jumped  to  their  feet  and  made  tall  piles  of  all  the  things.  Off\n",
        "they went, not waiting for trays, balancing columns of plates, each with a bottle on\n",
        "the  top,  with  one  hand,  while  the  hobbit  ran  after  them  almost  squeaking  with\n",
        "fright:  \"please  be  careful!\"  and  \"please,  don't  trouble!  I  can  manage.\"  But  the\n",
        "dwarves only started to sing:\n",
        "\n",
        "\n",
        "Chip the glasses and crack the plates!\n",
        "Blunt the knives and bend the forks!\n",
        "That's what Bilbo Baggins hates-\n",
        "Smash the bottles and burn the corks!\n",
        "\n",
        "Cut the cloth and tread on the fat!\n",
        "Pour the milk on the pantry floor!\n",
        "Leave the bones on the bedroom mat!\n",
        "\n",
        "####-Splash the wine on every door!\n",
        "\n",
        "Dump the crocks in a boiling bawl;\n",
        "Pound them up with a thumping pole;\n",
        "And when you've finished, if any are whole,\n",
        "Send them down the hall to roll !\n",
        "\n",
        "That's what Bilbo Baggins hates!\n",
        "So, carefully! carefully with the plates!\n",
        "\n",
        "\n",
        "And  of  course  they  did  none  of  these  dreadful  things,  and  everything  was\n",
        "cleaned  and  put  away  safe  as  quick  as  lightning,  while  the  hobbit  was  turning\n",
        "round  and  round  in  the middle of  the kitchen  trying  to  see what  they were doing.\n",
        "Then  they  went  back,  and  found  Thorin  with  his  feet  on  the  fender  smoking  a\n",
        "pipe. He was  blowing  the most  enormous  smoke-rings, and wherever he  told one\n",
        "to  go,  it  went-up  the  chimney,  or  behind  the  clock  on  the  man-telpiece,  or  under\n",
        "the  table,  or  round  and  round  the  ceiling;  but  wherever  it  went  it  was  not  quick\n",
        "enough  to escape Gandalf. Pop! he  sent a  smaller  smoke-ring from his short clay-\n",
        "pipe  straight  through  each  one  of  Thorin's.  The  Gandalf's  smoke-ring  would  go\n",
        "green  and  come  back  to  hover  over  the  wizard's  head.  He  had  quite  a  cloud  of\n",
        "them  about  him  already,  and  in  the  dim  light  it  made  him  look  strange  and\n",
        "sorcerous. Bilbo stood still and watched-he loved smoke-rings-and then be blushed\n",
        "to think how proud he had been yesterday morning of the smoke-rings he had sent\n",
        "up the wind over The Hill.\n",
        "\"Now for some music!\" said Thorin. \"Bring out the instruments!\"\n",
        "Kili  and  Fili  rushed  for  their  bags  and  brought  back  little  fiddles; Dori, Nori,\n",
        "and Ori brought out flutes from somewhere inside their coats; Bombur produced a\n",
        "drum  from  the  hall; Bifur  and Bofur went  out  too,  and  came  back with  clarinets\n",
        "that they had left among the walking-sticks Dwalin and Balin said: \"Excuse me, I\n",
        "left  mine  in  the  porch!\"  \"Just  bring  mine  in  with  you,\"  said  Thorin.  They  came\n",
        "back with  viols  as  big  as  themselves,  and with  Thorin’s  harp wrapped  in  a  green\n",
        "cloth.  It was  a  beautiful  gold-en harp, and when Thorin struck it the music began\n",
        "all  at  once,  so  sudden  and  sweet  that Bilbo  forgot everything else, and was  swept\n",
        "away  into dark  lands under  strange moons,  far over The Water and very  far  from\n",
        "his hobbit-hole under The Hill.\n",
        "The dark came into the room from the little window that opened in the side of\n",
        "The  Hill;  the  firelight  flickered-it  was  April-and  still  they  played  on,  while  the\n",
        "shadow of Gandalf's beard wagged against the wall.\n",
        "\n",
        "####-The  dark  filled  all  the  room,  and  the  fire  died  down,  and  the  shadows  were\n",
        "lost,  and  still  they  played  on.  And  suddenly  first  one  and  then  another  began  to\n",
        "sing  as  they  played,  deep-throated  singing  of  the  dwarves  in  the  deep  places  of\n",
        "their ancient homes; and this is like a fragment of their song, if it can be like their\n",
        "song without their music.\n",
        "\n",
        "\n",
        "Far over the misty mountains cold\n",
        "To dungeons deep and caverns old\n",
        "We must away ere break of day\n",
        "To seek the pale enchanted gold.\n",
        "\n",
        "The dwarves of yore made mighty spells,\n",
        "While hammers fell like ringing bells\n",
        "In places deep, where dark things sleep,\n",
        "In hollow halls beneath the fells.\n",
        "\n",
        "For ancient king and elvish lord\n",
        "There many a gloaming golden hoard\n",
        "They shaped and wrought, and light they caught\n",
        "To hide in gems on hilt of sword.\n",
        "\n",
        "On silver necklaces they strung\n",
        "The flowering stars, on crowns they hung\n",
        "The dragon-fire, in twisted wire\n",
        "They meshed the light of moon and sun.\n",
        "\n",
        "Far over the misty mountains cold\n",
        "To dungeons deep and caverns old\n",
        "We must away, ere break of day,\n",
        "To claim our long-forgotten gold.\n",
        "\n",
        "Goblets they carved there for themselves\n",
        "And harps of gold; where no man delves\n",
        "There lay they long, and many a song\n",
        "Was sung unheard by men or elves.\n",
        "\n",
        "The pines were roaring on the height,\n",
        "The winds were moaning in the night.\n",
        "The fire was red, it flaming spread;\n",
        "The trees like torches biased with light,\n",
        "\n",
        "The bells were ringing in the dale\n",
        "And men looked up with faces pale;\n",
        "\n",
        "####-The dragon's ire more fierce than fire\n",
        "Laid low their towers and houses frail.\n",
        "\n",
        "The mountain smoked beneath the moon;\n",
        "The dwarves, they heard the tramp of doom.\n",
        "They fled their hall to dying -fall\n",
        "Beneath his feet, beneath the moon.\n",
        "\n",
        "Far over the misty mountains grim\n",
        "To dungeons deep and caverns dim\n",
        "We must away, ere break of day,\n",
        "To win our harps and gold from him!\n",
        "\n",
        "\n",
        "As they sang the hobbit felt the love of beautiful things made by hands and by\n",
        "cunning and by magic moving through him, a fierce and jealous love, the desire of\n",
        "the  hearts  of  dwarves.  Then  something  Tookish  woke  up  inside  him,  and  he\n",
        "wished  to  go  and  see  the  great  mountains,  and  hear  the  pine-trees  and  the\n",
        "waterfalls, and explore the caves, and wear a sword instead of a walking-stick. He\n",
        "looked  out  of  the  window.  The  stars  were  out  in  a  dark  sky  above  the  trees.  He\n",
        "thought  of  the  jewels  of  the  dwarves  shining  in  dark  caverns.  Suddenly  in  the\n",
        "wood  beyond  The  Water  a  flame  leapt  up--probably  somebody  lighting  a  wood-\n",
        "fire-and he thought of plundering dragons settling on his quiet Hill and kindling it\n",
        "all  to  flames. He  shuddered;  and  very  quickly  he was  plain Mr.  Baggins  of  Bag-\n",
        "End, Under-Hill, again.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "uYPkdUHDJxpn"
      },
      "outputs": [],
      "source": [
        "# Tokenize o texto\n",
        "tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "\n",
        "# Constrói o vocabulário\n",
        "word_counts = Counter(tokens)\n",
        "vocab = sorted(word_counts.keys())\n",
        "vocab_size = len(vocab)\n",
        "word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "idx2word = {idx: word for word, idx in word2idx.items()}\n",
        "\n",
        "# Converte tokens para índices\n",
        "indices = [word2idx[word] for word in tokens]\n",
        "\n",
        "# Gera as sequências\n",
        "sequence_length = 5\n",
        "inputs = []\n",
        "targets = []\n",
        "\n",
        "for i in range(len(indices) - sequence_length):\n",
        "    inputs.append(indices[i:i+sequence_length])\n",
        "    targets.append(indices[i+1:i+sequence_length+1])\n",
        "\n",
        "# Converte para tensores\n",
        "inputs = torch.tensor(inputs, dtype=torch.long)\n",
        "targets = torch.tensor(targets, dtype=torch.long)\n",
        "\n",
        "# Cria o dataset e o dataloader\n",
        "batch_size = 2\n",
        "dataset = TensorDataset(inputs, targets)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "dooOPEMz6N2G"
      },
      "outputs": [],
      "source": [
        "d_model = 64\n",
        "num_heads = 8\n",
        "d_ff = 256\n",
        "num_layers = 2\n",
        "dropout = 0.1\n",
        "max_len = 500\n",
        "\n",
        "model = Decoder(vocab_size, d_model, num_heads, num_layers, d_ff, max_len, dropout).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "_tCplbXf9Vsk"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "arK6qCEb9Y7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3370bd0f-3004-4f91-8e6f-6604ffc048a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 5.2329\n",
            "Epoch 2/10, Loss: 3.6422\n",
            "Epoch 3/10, Loss: 2.9080\n",
            "Epoch 4/10, Loss: 2.5085\n",
            "Epoch 5/10, Loss: 2.2195\n",
            "Epoch 6/10, Loss: 2.0167\n",
            "Epoch 7/10, Loss: 1.8574\n",
            "Epoch 8/10, Loss: 1.7387\n",
            "Epoch 9/10, Loss: 1.6534\n",
            "Epoch 10/10, Loss: 1.5759\n"
          ]
        }
      ],
      "source": [
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch_inputs, batch_targets in dataloader:\n",
        "        batch_inputs = batch_inputs.to(device)\n",
        "        batch_targets = batch_targets.to(device)\n",
        "\n",
        "        batch_size, seq_len = batch_inputs.size()\n",
        "        mask = create_causal_mask(seq_len).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_inputs, trg_mask=mask)\n",
        "        outputs = outputs.view(-1, vocab_size)\n",
        "        batch_targets = batch_targets.view(-1)\n",
        "\n",
        "        loss = criterion(outputs, batch_targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "QjL7D4e9-DUT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "605b9103-8780-40dc-9a18-409840af69be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto gerado:\n",
            "the hobbit bedrooms bathrooms cellars pantries lots of sword of his dark sky blue his house by this time so far\n"
          ]
        }
      ],
      "source": [
        "def generate_text(model, start_tokens, generate_length, idx2word, device):\n",
        "    model.eval()\n",
        "    tokens = start_tokens.copy()\n",
        "\n",
        "    for _ in range(generate_length):\n",
        "        input_tensor = torch.tensor([tokens], dtype=torch.long).to(device)\n",
        "        seq_len = input_tensor.size(1)\n",
        "        mask = create_causal_mask(seq_len).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(input_tensor, trg_mask=mask)\n",
        "            next_token_logits = output[:, -1, :]\n",
        "            next_token = torch.argmax(next_token_logits, dim=-1).item()\n",
        "            tokens.append(next_token)\n",
        "    words = [idx2word[idx] for idx in tokens]\n",
        "    return ' '.join(words)\n",
        "\n",
        "start_word = \"the\"\n",
        "start_token = [word2idx[start_word]]\n",
        "generated_text = generate_text(model, start_token, generate_length=20, idx2word=idx2word, device=device)\n",
        "print(\"Texto gerado:\")\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBrRV2tyQtKE"
      },
      "source": [
        "## Exercícios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VO8bj7iQtKE"
      },
      "source": [
        "### Exercício 1\n",
        "Implemente um módulo que utilize apenas o módulo Encoder para a classificação de texto em `num_classes` classes. Para a obtenção do vetor de embedding de toda a sequência que será enviado para a cabeça de classificação, faça um pooling de média através da dimensão de sequência."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfU4CT08QtKE"
      },
      "source": [
        "### Exercício 2\n",
        "Vamos implementar um modelo baseado em stack de decoders. Uma vez que não é necessário cross-attention, pois não há encoders, utilize o módulo `EncoderLayer`. O tamanho do vocabulário deverá ser de 50257, o tamanho dos embeddings de 768, 12 cabeças de atenção, 12 camadas, dimensão da camada feedforward de 3072 e tamanho máximo de sequência 1024. Em seguida, teste com valores aleatórios simulando uma sequência de tokens."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}