{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "P8iOOH4cG_Lf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dc26eac-117b-4187-ef2e-32d930ca7b70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PK_Hpl9KG_Lg"
      },
      "source": [
        "## Modelos"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install langchain-openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyV6H-qHAsLG",
        "outputId": "20d918bf-5dcb-42da-fddd-84a4a852d516"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.2.14-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting langchain-core<0.4.0,>=0.3.27 (from langchain-openai)\n",
            "  Downloading langchain_core-0.3.28-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting openai<2.0.0,>=1.58.1 (from langchain-openai)\n",
            "  Downloading openai-1.58.1-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-openai) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-openai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-openai) (0.2.3)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-openai) (24.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-openai) (2.10.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-openai) (9.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-openai) (4.12.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.58.1->langchain-openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.58.1->langchain-openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain-openai) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain-openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain-openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.27->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-openai) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-openai) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.27->langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.27->langchain-openai) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.2.3)\n",
            "Downloading langchain_openai-0.2.14-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.28-py3-none-any.whl (411 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.58.1-py3-none-any.whl (454 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.3/454.3 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken, openai, langchain-core, langchain-openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.57.4\n",
            "    Uninstalling openai-1.57.4:\n",
            "      Successfully uninstalled openai-1.57.4\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.25\n",
            "    Uninstalling langchain-core-0.3.25:\n",
            "      Successfully uninstalled langchain-core-0.3.25\n",
            "Successfully installed langchain-core-0.3.28 langchain-openai-0.2.14 openai-1.58.1 tiktoken-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4L4wFqB8G_Lg"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FZlKiOdSG_Lh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7eb5b9f7-2283-4498-942c-11f61375b7ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='Olá! Como posso ajudar você hoje?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 8, 'total_tokens': 17, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0aa8d3e20b', 'finish_reason': 'stop', 'logprobs': None} id='run-3840db13-85e5-40ee-af9a-51d40eea6749-0' usage_metadata={'input_tokens': 8, 'output_tokens': 9, 'total_tokens': 17, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
          ]
        }
      ],
      "source": [
        "response = llm.invoke(\"Olá\")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3DgRBemG_Lh"
      },
      "source": [
        "## Prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okVH6kE5G_Lh"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yz72lmAkG_Lh"
      },
      "source": [
        "### Templates Simples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "uAzP8Pg4G_Lh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7fafb02-d66c-410d-e6eb-b9452c15af6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "messages=[HumanMessage(content='Traduza o seguinte texto para português: Artificial Intelligence is the future!', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(\"Traduza o seguinte texto para português: {texto}\")\n",
        "\n",
        "prompt = prompt_template.invoke({\"texto\": \"Artificial Intelligence is the future!\"})\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "g3rWIeEVG_Li",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "099530f6-4573-47d0-af4d-2e1095a6f9e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A Inteligência Artificial é o futuro!\n"
          ]
        }
      ],
      "source": [
        "response = llm.invoke(prompt)\n",
        "\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fv_GQRfbG_Li"
      },
      "source": [
        "### Templates de Mensagens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "IB10RWfwG_Li",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbb6cccd-1e3a-43e8-aba4-b6e66afae382"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='Olá, como você está?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 36, 'total_tokens': 43, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0aa8d3e20b', 'finish_reason': 'stop', 'logprobs': None} id='run-33987138-5cbb-420e-a4be-4297eb95f0a0-0' usage_metadata={'input_tokens': 36, 'output_tokens': 7, 'total_tokens': 43, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"Você é um tradutor de inglês para português. Traduza as mensagens que forem enviadas.\"),\n",
        "    HumanMessage(content=\"Hello, how are you?\"),\n",
        "]\n",
        "\n",
        "# messages = [\n",
        "#     (\"system\", \"Você é um tradutor de inglês para português. Traduza as mensagens que forem enviadas.\"),\n",
        "#     (\"human\", \"Hello, how are you?\"),\n",
        "# ]\n",
        "\n",
        "response = llm.invoke(messages)\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "NcWw_lBgG_Lj"
      },
      "outputs": [],
      "source": [
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"Você é um tradutor de {lingua_origem} para {lingua_destino}. Traduza as mensagens que forem enviadas.\"),\n",
        "        (\"user\", \"{texto}\")\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "vNt_aw_3G_Lj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fad4b5d-07ff-484a-b140-3f658c808c23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "messages=[SystemMessage(content='Você é um tradutor de inglês para português. Traduza as mensagens que forem enviadas.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello, how are you?', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ],
      "source": [
        "prompt = prompt_template.invoke({\n",
        "    \"lingua_origem\": \"inglês\",\n",
        "    \"lingua_destino\": \"português\",\n",
        "    \"texto\": \"Hello, how are you?\"\n",
        "})\n",
        "\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "xYfUg8ImG_Lj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfc5e72b-092a-4cc3-885d-9c820ed44afa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Olá, como você está?\n"
          ]
        }
      ],
      "source": [
        "response = llm.invoke(prompt)\n",
        "\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MZpuOgJG_Lj"
      },
      "source": [
        "### Parsers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "mUUBvZa9G_Lk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbdade71-4a01-4785-ec53-db21ed71027b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resposta:\n",
            "content='A capital do Rio Grande do Norte é Natal.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 16, 'total_tokens': 27, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0aa8d3e20b', 'finish_reason': 'stop', 'logprobs': None} id='run-4d2a550d-1b33-466f-a77c-d95d2295169f-0' usage_metadata={'input_tokens': 16, 'output_tokens': 11, 'total_tokens': 27, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "\n",
            "Saída do parser:\n",
            "A capital do Rio Grande do Norte é Natal.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "str_parser = StrOutputParser()\n",
        "\n",
        "response = llm.invoke(\"Qual a capital do Rio Grande do Norte?\")\n",
        "output = str_parser.invoke(response)\n",
        "\n",
        "print(\"Resposta:\")\n",
        "print(response)\n",
        "print()\n",
        "print(\"Saída do parser:\")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "S5TQaMdwG_Lk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf22c441-0396-4dea-aee3-114f096fa3aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resposta:\n",
            "content='Aqui está a representação das massas e cargas das partículas que constituem um átomo no formato JSON:\\n\\n```json\\n{\\n    \"protão\": {\\n        \"massa\": \"1.6726 x 10^-27 kg\",\\n        \"carga\": \"+1 e\"\\n    },\\n    \"neutrão\": {\\n        \"massa\": \"1.6750 x 10^-27 kg\",\\n        \"carga\": \"0 e\"\\n    },\\n    \"electrão\": {\\n        \"massa\": \"9.1094 x 10^-31 kg\",\\n        \"carga\": \"-1 e\"\\n    }\\n}\\n```\\n\\nNeste JSON:\\n\\n- O protão possui uma massa de aproximadamente \\\\(1.6726 \\\\times 10^{-27}\\\\) kg e uma carga positiva de \\\\(+1 e\\\\).\\n- O neutrão tem uma massa de cerca de \\\\(1.6750 \\\\times 10^{-27}\\\\) kg e não possui carga (carga de \\\\(0 e\\\\)).\\n- O electrão tem uma massa de aproximadamente \\\\(9.1094 \\\\times 10^{-31}\\\\) kg e uma carga negativa de \\\\(-1 e\\\\).' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 237, 'prompt_tokens': 38, 'total_tokens': 275, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0aa8d3e20b', 'finish_reason': 'stop', 'logprobs': None} id='run-60a69c13-6e7b-49a6-afaf-59a583f5e5cb-0' usage_metadata={'input_tokens': 38, 'output_tokens': 237, 'total_tokens': 275, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "\n",
            "Saída do parser:\n",
            "{'protão': {'massa': '1.6726 x 10^-27 kg', 'carga': '+1 e'}, 'neutrão': {'massa': '1.6750 x 10^-27 kg', 'carga': '0 e'}, 'electrão': {'massa': '9.1094 x 10^-31 kg', 'carga': '-1 e'}}\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "json_parser = JsonOutputParser()\n",
        "\n",
        "response = llm.invoke(\"Quais as massas e cargas das partículas que constituem o átomo? Responda no formato JSON em que cada chave seja o nome da partícula\")\n",
        "output = json_parser.invoke(response)\n",
        "\n",
        "print(\"Resposta:\")\n",
        "print(response)\n",
        "print()\n",
        "print(\"Saída do parser:\")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqzpSoJ9G_Lk"
      },
      "source": [
        "## Encadeamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "CUvAK_wJG_Lk"
      },
      "outputs": [],
      "source": [
        "chain = prompt_template | llm | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "z99NNexyG_Lk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbf4a726-3da4-48f9-84d7-cff4ec28d2d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¡Las playas de Recife tienen tiburones!\n"
          ]
        }
      ],
      "source": [
        "response = chain.invoke({\n",
        "    \"lingua_origem\": \"inglês\",\n",
        "    \"lingua_destino\": \"espanhol\",\n",
        "    \"texto\": \"As praias de Recife tem tubarões!\"\n",
        "})\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "R-zYf9vWG_Lk"
      },
      "outputs": [],
      "source": [
        "def translate(texto, lingua_origem, lingua_destino):\n",
        "    response = chain.invoke({\n",
        "        \"lingua_origem\": lingua_origem,\n",
        "        \"lingua_destino\": lingua_destino,\n",
        "        \"texto\": texto\n",
        "    })\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "_bBlO70AG_Ll",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1099307-dbbc-4cbf-850f-fe2c765c491a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¡Las playas de Recife tienen tiburones!\n"
          ]
        }
      ],
      "source": [
        "output = translate(\"The beaches of Recife have sharks!\", \"inglês\", \"espanhol\")\n",
        "\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8FFSCk0G_Ll"
      },
      "source": [
        "## Exercícios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad7-7vfVG_Ll"
      },
      "source": [
        "### Exercício 1\n",
        "Crie uma `chain` que a partir de um tópico informado pelo usuário, crie uma piada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "cVJdHJWEG_Ll",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7021baf7-6636-4e3d-8661-d577929d8967"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Por que o livro de cálculo se sentiu mal?\n",
            "\n",
            "Porque tinha muitos problemas!\n"
          ]
        }
      ],
      "source": [
        "prompt_template_ex1 = ChatPromptTemplate.from_template(\n",
        "    \"Você é um comediante experiente. Criar uma piada criativa e engraçada sobre {topico}. \\\n",
        "    Certifique-se de que a piada seja apropriada para todos os públicos.\"\n",
        ")\n",
        "chain = prompt_template_ex1 | llm | StrOutputParser()\n",
        "print(chain.invoke({\"topico\": \"Cálculo\"}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3it9s4tOG_Ll"
      },
      "source": [
        "### Exercício 2\n",
        "Crie uma `chain` que classifique o sentimento de um texto de entrada em positivo, neutro ou negativo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "LB2TWJlJG_Ll",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "015213f7-60da-4034-8e58-98e646212674"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O sentimento do texto é classificado como Negativo.\n",
            "\n",
            "O sentimento do texto é classificado como Positivo.\n",
            "\n",
            "O sentimento do texto é classificado como Negativo. Embora haja menções a pratos que foram \"muito saborosos\", a insatisfação com as fritas e a declaração de que a pessoa não tem certeza se fará um pedido novamente indicam uma avaliação negativa geral da experiência.\n"
          ]
        }
      ],
      "source": [
        "prompt_template_ex2 = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"Você é um assistente inteligente especializado em análise de sentimentos. \\\n",
        "                    Analise o texto e classifique o sentimento como Positivo, Neutro ou Negativo.\"),\n",
        "        (\"user\", \"{texto}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt_template_ex2 | llm | StrOutputParser()\n",
        "\n",
        "texto1 = \"\"\"Disgusting food. Everything terrible cooked. We ordered hamburgers and\n",
        "            sweet potatoes and these are the pictures. The meat was totally raw and\n",
        "            the sweet potatoes extremely greasy and overcooked done with old oil for sure.\n",
        "            In short, food to make you sick. And the worst things is that we called to\n",
        "            complain about this and they told us they couldn’t do anything. They don’t\n",
        "            even tried to fix the problem asking us for pictures or asking we went there\n",
        "            to show them.\"\"\"\n",
        "\n",
        "texto2 = \"\"\"Everything was perfect. First the service, was really good, they helped us with\n",
        "            the food of our baby and they were very attentive. The burguer was really tasty\n",
        "            (blind burguer) and I will be back to taste the smokey burguer. Also salads are\n",
        "            very tasty. With coffes and fries total price was 42 euros, so totally recommended.\n",
        "         \"\"\"\n",
        "\n",
        "texto3 = \"\"\"We had brisket, chicken satay and a smokey burger. The brisket and chicken satay\n",
        "            were very tasty. The burger was ok, nothing special. Only the fries were not ok.\n",
        "            To be honest, we picked up the dishes. Next time I eat there the fries will be better.\n",
        "            Would I pick up from there again, I don't think so.\"\"\"\n",
        "\n",
        "print(chain.invoke({\"texto\": texto1}))\n",
        "print()\n",
        "print(chain.invoke({\"texto\": texto2}))\n",
        "print()\n",
        "print(chain.invoke({\"texto\": texto3}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jI1P22I1G_Ll"
      },
      "source": [
        "### Exercício 3\n",
        "Crie uma `chain` que gere o código de uma função Python de acordo com a descrição do usuário."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "2Y_lhlypG_Ll",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bad0594f-11ef-47d2-8605-db2139cfa86d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```python\n",
            "import math\n",
            "\n",
            "def calcular_area_cone(raio, altura):\n",
            "    \"\"\"\n",
            "    Calcula a área total de um cone.\n",
            "\n",
            "    Parâmetros:\n",
            "    raio (float): O raio da base do cone.\n",
            "    altura (float): A altura do cone.\n",
            "\n",
            "    Retorna:\n",
            "    float: A área total do cone.\n",
            "    \"\"\"\n",
            "    # Cálculo da área da base do cone (círculo)\n",
            "    area_base = math.pi * raio**2\n",
            "    \n",
            "    # Cálculo da geratriz do cone\n",
            "    geratriz = math.sqrt(raio**2 + altura**2)\n",
            "    \n",
            "    # Cálculo da área lateral do cone\n",
            "    area_lateral = math.pi * raio * geratriz\n",
            "    \n",
            "    # Área total é a soma da área da base e da área lateral\n",
            "    area_total = area_base + area_lateral\n",
            "    \n",
            "    return area_total\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "prompt_template_ex3 = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"Você é um programados experiente em Python. \\\n",
        "                    Forneça apenas o código da função formatado corretamente e com comentários.\"),\n",
        "        (\"user\", \"{description}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt_template_ex3 | llm | StrOutputParser()\n",
        "\n",
        "description = \"Gere uma função que calcule a área de um cone\"\n",
        "\n",
        "print(chain.invoke({\"description\": description}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGmivo4EG_Ll"
      },
      "source": [
        "### Exercício 4\n",
        "Crie uma `chain` que explique de forma simplificada um tópico geral fornecido pelo usuário e, em seguida, traduza a explicação para inglês. Utilize dois templates encadeados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "yGaLBgZPG_Ll",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2b75f1d-87ea-4858-a968-8f87757a0b04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metallic materials are substances primarily made of metals, characterized by properties such as good electrical and thermal conductivity, strength, malleability (the ability to be shaped), and ductility (the ability to be stretched). The most common metals include iron, aluminum, copper, and steel. These materials are widely used in construction, manufacturing of machinery, utensils, vehicles, and various industries due to their durability and versatility. Additionally, metallic materials can be treated, and alloys can be formed to enhance their properties, such as resistance to corrosion and wear.\n"
          ]
        }
      ],
      "source": [
        "prompt_template_ex4_1 = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"Explique de forma simplificada o tópico informado.\"),\n",
        "        (\"user\", \"{topic}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "prompt_template_ex4_2 = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"Traduza a explicação para o inglês.\"),\n",
        "        (\"user\", \"{explanation}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt_template_ex4_1 | llm | StrOutputParser() \\\n",
        "      | prompt_template_ex4_2 | llm | StrOutputParser()\n",
        "\n",
        "print(chain.invoke({\"topic\": \"Materiais metálicos\"}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtgY56VtG_Lm"
      },
      "source": [
        "### Exercício 5 - Desafio\n",
        "Crie uma `chain` que responda perguntas sobre o CESAR School."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template_ex5 = ChatPromptTemplate.from_template(\"Responda a pergunta sobre o CESAR School: {question}\")\n",
        "chain = prompt_template_ex5 | llm | StrOutputParser()\n",
        "print(chain.invoke({\"question\": \"Quais cursos de pós graduação existem?\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrmTPeHoLgCU",
        "outputId": "a9c48a61-baf6-4f81-a29c-f85a35b758b6"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O CESAR School oferece uma variedade de cursos de pós-graduação focados em áreas como design, tecnologia e inovação. Embora eu não tenha acesso a informações em tempo real, os cursos comumente oferecidos incluem:\n",
            "\n",
            "1. **Pós-graduação em Design de Interação**\n",
            "2. **Pós-graduação em Gestão de Produtos**\n",
            "3. **Pós-graduação em Ciência de Dados**\n",
            "4. **Pós-graduação em Desenvolvimento de Software**\n",
            "5. **Pós-graduação em Inovação e Empreendedorismo**\n",
            "\n",
            "Recomendo visitar o site oficial do CESAR School ou entrar em contato diretamente com a instituição para obter informações atualizadas sobre os cursos disponíveis, pois a oferta pode mudar ao longo do tempo.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}